{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sfgeekgit/lesswrongDataDzppg/blob/main/lesswrong_challenge_colonizing_the_superhypersphere.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# lesswrong challenge: Colonizing the SuperHyperSphere"
      ],
      "metadata": {
        "id": "V8n_vM3jIitW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "problem: https://www.lesswrong.com/posts/Rpjrwspx2QZuHbmPE/d-and-d-sci-fi-colonizing-the-superhypersphere-evaluation\n",
        "\n",
        "submit: https://h-b-p.github.io/d-and-d-sci-SuperHyperSphere/"
      ],
      "metadata": {
        "id": "oB3fu4tcIgHF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## setup"
      ],
      "metadata": {
        "id": "aktjQ_0E0BWR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/sfgeekgit/lesswrongDataDzppg/main/cleared_sites_formated.csv\n",
        "!wget https://raw.githubusercontent.com/sfgeekgit/lesswrongDataDzppg/main/measured_data.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LOkR7Lp1B_6b",
        "outputId": "acc75e28-97a4-4b55-f703-703fc4e8c5bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-01-25 22:47:33--  https://raw.githubusercontent.com/sfgeekgit/lesswrongDataDzppg/main/cleared_sites_formated.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 11480603 (11M) [text/plain]\n",
            "Saving to: ‘cleared_sites_formated.csv’\n",
            "\n",
            "cleared_sites_forma 100%[===================>]  10.95M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2024-01-25 22:47:33 (105 MB/s) - ‘cleared_sites_formated.csv’ saved [11480603/11480603]\n",
            "\n",
            "--2024-01-25 22:47:33--  https://raw.githubusercontent.com/sfgeekgit/lesswrongDataDzppg/main/measured_data.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1175467 (1.1M) [text/plain]\n",
            "Saving to: ‘measured_data.csv’\n",
            "\n",
            "measured_data.csv   100%[===================>]   1.12M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2024-01-25 22:47:34 (19.3 MB/s) - ‘measured_data.csv’ saved [1175467/1175467]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "82HUMHLYzpwS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KQTuTq9u_WBQ"
      },
      "outputs": [],
      "source": [
        "# import data\n",
        "unlabeled = pd.read_csv('cleared_sites_formated.csv')\n",
        "labeled = pd.read_csv('measured_data.csv')\n",
        "n_features = labeled.shape[1] - 2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# split into training and test sets\n",
        "train_data2, test_data = train_test_split(labeled, test_size =.1, random_state = 1)\n",
        "train_labels = train_data2['ZPPG_Performance']\n",
        "train_data = train_data2.drop(['ZPPG_id', 'ZPPG_Performance'], axis=1)\n",
        "\n",
        "print(f'train data: {len(train_data)}')\n",
        "print(f'test data:  {len(test_data)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xKtElfWrAiqi",
        "outputId": "d5b29791-b579-4255-ac9e-f88c555a8e01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train data: 9366\n",
            "test data:  1041\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## utils"
      ],
      "metadata": {
        "id": "-RtIRD5cz948"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def evaluate(model, dataset):\n",
        "  model.eval()\n",
        "  val = dataset.drop(['ZPPG_id', 'ZPPG_Performance'], axis=1)\n",
        "  preds = model(torch.tensor(val.values).float().to(device))\n",
        "  labels = torch.tensor(dataset['ZPPG_Performance'].values).view(-1, 1).to(device)\n",
        "  diff = preds.detach() - labels\n",
        "  error = diff.abs().mean().item()\n",
        "  model.train()\n",
        "  return error"
      ],
      "metadata": {
        "id": "4_e6yDO-PVLk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def topn(model, dataset, n=15):\n",
        "  model.eval()\n",
        "  to_drop = ['ZPPG_id']\n",
        "  if 'ZPPG_Performance' in dataset: to_drop.append('ZPPG_Performance')\n",
        "  preds = model(torch.tensor(dataset.drop(to_drop, axis=1).values).float().to(device))\n",
        "  ids = dataset['ZPPG_id']\n",
        "  preds_id = list(zip(\n",
        "    preds.view(-1).tolist(),\n",
        "    ids.values.tolist()))\n",
        "  return sorted(preds_id, reverse=True)[:n]"
      ],
      "metadata": {
        "id": "90VqjJY46_3T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## model and train"
      ],
      "metadata": {
        "id": "8PS_EWz00EXo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "  def __init__(self, n_inputs=n_features, hidden=32, dropout=0.2):\n",
        "    super().__init__()\n",
        "    self.model = nn.Sequential(\n",
        "        nn.Linear(n_inputs, hidden),\n",
        "        nn.BatchNorm1d(hidden),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(dropout),\n",
        "        nn.Linear(hidden, hidden),\n",
        "        nn.BatchNorm1d(hidden),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(dropout),\n",
        "        nn.Linear(hidden, hidden),\n",
        "        nn.BatchNorm1d(hidden),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(dropout),\n",
        "        nn.Linear(hidden, hidden),\n",
        "        nn.BatchNorm1d(hidden),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(dropout),\n",
        "        nn.Linear(hidden, hidden),\n",
        "        nn.BatchNorm1d(hidden),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(dropout),\n",
        "        nn.Linear(hidden, hidden),\n",
        "        nn.BatchNorm1d(hidden),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(dropout),\n",
        "        nn.Linear(hidden, hidden),\n",
        "        nn.BatchNorm1d(hidden),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(dropout),\n",
        "        nn.Linear(hidden, 1),\n",
        "    )\n",
        "  def forward(self, x):\n",
        "    output = self.model(x)\n",
        "    return output"
      ],
      "metadata": {
        "id": "eWGrZfBgAjlJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MLP().to(device)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=3e-4, weight_decay=0.01)"
      ],
      "metadata": {
        "id": "6ba5xzovGe0u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, epochs=10000):\n",
        "  model.train()\n",
        "  labels = torch.tensor(labeled['ZPPG_Performance'].values).float().view(-1, 1).to(device)\n",
        "  data = labeled.drop(['ZPPG_id', 'ZPPG_Performance'], axis=1)\n",
        "  data = torch.tensor(data.values).float().to(device)\n",
        "  # data = torch.tensor(train_data.values).float().to(device)\n",
        "  # labels = torch.tensor(train_labels.values).float().view(-1, 1).to(device)\n",
        "  for epoch in range(epochs):\n",
        "    predictions = model(data)\n",
        "    loss = F.mse_loss(predictions, labels)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if epoch % 200 == 0:\n",
        "      error_test = error_train = evaluate(model, labeled)\n",
        "      # error_test = evaluate(model, test_data)\n",
        "      # error_train = evaluate(model, train_data2)\n",
        "      print(f'{epoch=:5} {loss.item()=:7.4f} {error_test=:7.4f} {error_train=:7.4f}')\n",
        "  model.eval()\n",
        "\n",
        "train(model)"
      ],
      "metadata": {
        "id": "vAdJbaa_AuQ-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6553040-4c8f-432d-cca1-8d3bde99a285"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch=    0 loss.item()= 0.0009 error_test= 0.1059 error_train= 0.1059\n",
            "epoch=  200 loss.item()= 0.0009 error_test= 0.1059 error_train= 0.1059\n",
            "epoch=  400 loss.item()= 0.0010 error_test= 0.1056 error_train= 0.1056\n",
            "epoch=  600 loss.item()= 0.0009 error_test= 0.1053 error_train= 0.1053\n",
            "epoch=  800 loss.item()= 0.0009 error_test= 0.1046 error_train= 0.1046\n",
            "epoch= 1000 loss.item()= 0.0009 error_test= 0.1045 error_train= 0.1045\n",
            "epoch= 1200 loss.item()= 0.0009 error_test= 0.1059 error_train= 0.1059\n",
            "epoch= 1400 loss.item()= 0.0009 error_test= 0.1062 error_train= 0.1062\n",
            "epoch= 1600 loss.item()= 0.0010 error_test= 0.1070 error_train= 0.1070\n",
            "epoch= 1800 loss.item()= 0.0009 error_test= 0.1065 error_train= 0.1065\n",
            "epoch= 2000 loss.item()= 0.0009 error_test= 0.1069 error_train= 0.1069\n",
            "epoch= 2200 loss.item()= 0.0009 error_test= 0.1064 error_train= 0.1064\n",
            "epoch= 2400 loss.item()= 0.0009 error_test= 0.1050 error_train= 0.1050\n",
            "epoch= 2600 loss.item()= 0.0010 error_test= 0.1051 error_train= 0.1051\n",
            "epoch= 2800 loss.item()= 0.0009 error_test= 0.1048 error_train= 0.1048\n",
            "epoch= 3000 loss.item()= 0.0010 error_test= 0.1077 error_train= 0.1077\n",
            "epoch= 3200 loss.item()= 0.0010 error_test= 0.1049 error_train= 0.1049\n",
            "epoch= 3400 loss.item()= 0.0009 error_test= 0.1048 error_train= 0.1048\n",
            "epoch= 3600 loss.item()= 0.0009 error_test= 0.1054 error_train= 0.1054\n",
            "epoch= 3800 loss.item()= 0.0009 error_test= 0.1058 error_train= 0.1058\n",
            "epoch= 4000 loss.item()= 0.0009 error_test= 0.1067 error_train= 0.1067\n",
            "epoch= 4200 loss.item()= 0.0009 error_test= 0.1063 error_train= 0.1063\n",
            "epoch= 4400 loss.item()= 0.0009 error_test= 0.1069 error_train= 0.1069\n",
            "epoch= 4600 loss.item()= 0.0009 error_test= 0.1045 error_train= 0.1045\n",
            "epoch= 4800 loss.item()= 0.0010 error_test= 0.1070 error_train= 0.1070\n",
            "epoch= 5000 loss.item()= 0.0009 error_test= 0.1054 error_train= 0.1054\n",
            "epoch= 5200 loss.item()= 0.0009 error_test= 0.1037 error_train= 0.1037\n",
            "epoch= 5400 loss.item()= 0.0009 error_test= 0.1055 error_train= 0.1055\n",
            "epoch= 5600 loss.item()= 0.0009 error_test= 0.1073 error_train= 0.1073\n",
            "epoch= 5800 loss.item()= 0.0009 error_test= 0.1074 error_train= 0.1074\n",
            "epoch= 6000 loss.item()= 0.0009 error_test= 0.1054 error_train= 0.1054\n",
            "epoch= 6200 loss.item()= 0.0009 error_test= 0.1070 error_train= 0.1070\n",
            "epoch= 6400 loss.item()= 0.0009 error_test= 0.1072 error_train= 0.1072\n",
            "epoch= 6600 loss.item()= 0.0009 error_test= 0.1061 error_train= 0.1061\n",
            "epoch= 6800 loss.item()= 0.0009 error_test= 0.1061 error_train= 0.1061\n",
            "epoch= 7000 loss.item()= 0.0010 error_test= 0.1066 error_train= 0.1066\n",
            "epoch= 7200 loss.item()= 0.0009 error_test= 0.1051 error_train= 0.1051\n",
            "epoch= 7400 loss.item()= 0.0010 error_test= 0.1063 error_train= 0.1063\n",
            "epoch= 7600 loss.item()= 0.0009 error_test= 0.1062 error_train= 0.1062\n",
            "epoch= 7800 loss.item()= 0.0010 error_test= 0.1067 error_train= 0.1067\n",
            "epoch= 8000 loss.item()= 0.0009 error_test= 0.1061 error_train= 0.1061\n",
            "epoch= 8200 loss.item()= 0.0009 error_test= 0.1069 error_train= 0.1069\n",
            "epoch= 8400 loss.item()= 0.0010 error_test= 0.1066 error_train= 0.1066\n",
            "epoch= 8600 loss.item()= 0.0009 error_test= 0.1043 error_train= 0.1043\n",
            "epoch= 8800 loss.item()= 0.0009 error_test= 0.1043 error_train= 0.1043\n",
            "epoch= 9000 loss.item()= 0.0009 error_test= 0.1053 error_train= 0.1053\n",
            "epoch= 9200 loss.item()= 0.0009 error_test= 0.1051 error_train= 0.1051\n",
            "epoch= 9400 loss.item()= 0.0009 error_test= 0.1060 error_train= 0.1060\n",
            "epoch= 9600 loss.item()= 0.0009 error_test= 0.1055 error_train= 0.1055\n",
            "epoch= 9800 loss.item()= 0.0009 error_test= 0.1059 error_train= 0.1059\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## eval"
      ],
      "metadata": {
        "id": "1nDTgt3p0Jgm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# predict solution\n",
        "topn(model, unlabeled)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tLl-ai-v1uMU",
        "outputId": "12d8182e-9fde-47b9-83b3-ebba0640113c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0.7313426733016968, 23565),\n",
              " (0.7297401428222656, 93762),\n",
              " (0.7287615537643433, 96286),\n",
              " (0.7213119268417358, 107278),\n",
              " (0.7154839038848877, 53987),\n",
              " (0.714455246925354, 88956),\n",
              " (0.7140936851501465, 80395),\n",
              " (0.7107514142990112, 94408),\n",
              " (0.7104818820953369, 94304),\n",
              " (0.7085078954696655, 905),\n",
              " (0.7079377174377441, 94942),\n",
              " (0.7079243659973145, 38055),\n",
              " (0.7075937986373901, 104260),\n",
              " (0.7075873613357544, 58945),\n",
              " (0.7071173191070557, 11558)]"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# validation\n",
        "topn(model, test_data)\n",
        "topn(model, train_data2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gqh4i9be19PY",
        "outputId": "81646972-c4e3-4ce9-ab81-7afee8518e6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(1.8702857494354248, 3412),\n",
              " (1.8266371488571167, 2695),\n",
              " (1.8240541219711304, 7487),\n",
              " (1.8221070766448975, 2509),\n",
              " (1.821927547454834, 3773),\n",
              " (1.8169137239456177, 8721),\n",
              " (1.8135265111923218, 5386),\n",
              " (1.8075464963912964, 5751),\n",
              " (1.8061460256576538, 7243),\n",
              " (1.7997924089431763, 2004),\n",
              " (1.797755241394043, 5072),\n",
              " (1.7943958044052124, 4148),\n",
              " (1.7933684587478638, 6989),\n",
              " (1.791631817817688, 7146),\n",
              " (1.7850587368011475, 5667)]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    }
  ]
}